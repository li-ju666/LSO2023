{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "The two main goals of this exercise are:\n",
    "1. Familiarizing yourself with Numpy and scipy.optimize.\n",
    "2. Seeing the difference the choice of solver can make. \n",
    "\n",
    "### Setting: Least-squares\n",
    "Let $A\\in \\mathbb{R}^{m\\times n}$, and consider the least-squares problem\n",
    "$$\n",
    "\\underset{x}{\\text{minimize }} \\frac{1}{2m}\\|Ax - b\\|_2^2.\n",
    "$$\n",
    "The solution is known in closed form:\n",
    "$$\n",
    "x^*=(A^\\top A)^{-1}A^\\top b.\n",
    "$$\n",
    "In this exercise, your task is to solve the least-squares problem in a number of different ways.\n",
    "\n",
    "But first we import the necessary modules and define some utility code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import wraps\n",
    "from time import time\n",
    "\n",
    "# Utility code for timing\n",
    "def timing(f):\n",
    "    @wraps(f)\n",
    "    def wrap(*args, **kw):\n",
    "        ts = time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time()\n",
    "        return result, te-ts \n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: computational linear algebra\n",
    "Compute the closed-form solution using \n",
    "1. Gaussian elimination on the normal equations, \n",
    "2. explicit inversion of $A^\\top A$, and \n",
    "3. the pseudo-inverse.\n",
    "\n",
    "Compare the computation times as as you vary $m$ and $n$ using the code below. Explain what happens when $m < n$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing\n",
    "def gaussian_elimination(A,b):\n",
    "  raise NotImplementedError(\"Replace with your implementation!\")\n",
    "\n",
    "@timing\n",
    "def explicit_inv(A, b):\n",
    "  raise NotImplementedError(\"Replace with your implementation!\")\n",
    "\n",
    "@timing\n",
    "def pseudo_inv(A, b):\n",
    "  raise NotImplementedError(\"Replace with your implementation!\")\n",
    "\n",
    "# Verify that the above methods agree\n",
    "rng = np.random.default_rng(seed=42)\n",
    "A = rng.normal(size=(10, 3))\n",
    "b = rng.normal(size=(10, 1))\n",
    "assert np.allclose(gaussian_elimination(A, b)[0], explicit_inv(A, b)[0])\n",
    "assert np.allclose(gaussian_elimination(A, b)[0], pseudo_inv(A, b)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_exponents = 3 # Going higher than 5 might take a long time and a lot of memory\n",
    "m_arr = 2 ** (10 + np.arange(n_exponents))\n",
    "n_arr = 2 ** (8 + np.arange(n_exponents))\n",
    "print(\"m_arr:\\t{}\\nn_arr:\\t{}\".format(m_arr, n_arr))\n",
    "\n",
    "\n",
    "t_gaussian = np.zeros((n_exponents, n_exponents))\n",
    "t_inv = np.zeros_like(t_gaussian)\n",
    "t_pinv = np.zeros_like(t_gaussian)\n",
    "log2_mn2 = np.zeros_like(t_gaussian) # For the complexity plots below\n",
    "\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "for i, m in enumerate(m_arr):\n",
    "  for j, n in enumerate(n_arr):\n",
    "    A = rng.normal(size=(m, n))\n",
    "    b = rng.normal(size=(m, 1))\n",
    "    log2_mn2[i, j] = np.log2(m) + 2 * np.log2(n)\n",
    "\n",
    "    x_gaussian, t_gaussian[i, j] = gaussian_elimination(A, b)\n",
    "    x_inv, t_inv[i, j] = explicit_inv(A, b)\n",
    "    x_pinv, t_pinv[i, j] = pseudo_inv(A, b)\n",
    "\n",
    "    if not np.allclose(x_gaussian, x_inv):\n",
    "      print(\"Gaussian elimination and explicit inverse don't agree for m={} and n={}.\".format(m, n))\n",
    "    if not np.allclose(x_gaussian, x_pinv):\n",
    "      print(\"Gaussian elimination and pseudo-inverse don't agree for m={} and n={}.\".format(m, n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least-squares supposedly has complexity $\\mathcal{O}(mn^2)$. The below plot shows $\\log(mn^2)$ on the x-axis and $\\log(t)$ on the y-axis, for each method. Do the methods scale according to theory? Does that mean that they're equally good in practice? For example, what are the absolute times for the largest values of $(m, n)$ that you tried? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log2_mn2.flatten(), np.log2(t_gaussian.flatten()), '*', label=\"Gaussian elimination\")\n",
    "plt.plot(log2_mn2.flatten(), np.log2(t_inv.flatten()), '*', label=\"Explicit inverse\")\n",
    "plt.plot(log2_mn2.flatten(), np.log2(t_pinv.flatten()), '*', label=\"Pseudo-inverse\")\n",
    "plt.xlabel(\"log($mn^2$)\")\n",
    "plt.ylabel(\"log(time)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Solution\n",
    "Click the dots to see a solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@timing\n",
    "def gaussian_elimination(A,b):\n",
    "  x = np.linalg.solve(A.T @ A, A.T @ b)\n",
    "  return x\n",
    "\n",
    "@timing\n",
    "def explicit_inv(A, b):\n",
    "  AtA_inv = np.linalg.inv(A.T @ A)\n",
    "  x = AtA_inv @ (A.T @ b)\n",
    "  return x\n",
    "\n",
    "@timing\n",
    "def pseudo_inv(A, b):\n",
    "  A_pinv = np.linalg.pinv(A)\n",
    "  x = A_pinv @ b\n",
    "  return x\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(seed=42)\n",
    "A = rng.normal(size=(10, 3))\n",
    "b = rng.normal(size=(10, 1))\n",
    "assert np.allclose(gaussian_elimination(A, b)[0], explicit_inv(A, b)[0])\n",
    "assert np.allclose(gaussian_elimination(A, b)[0], pseudo_inv(A, b)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian elimination is both faster and numerically more stable than the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"On average, explicit inversion takes {:.2f} times longer than Gaussian elimination.\".format(np.mean(t_inv/t_gaussian)))\n",
    "print(\"On average, pseudo-inversion takes {:.2f} times longer than Gaussian elimination.\".format(np.mean(t_pinv/t_gaussian)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare absolute times for the largest (m, n)\n",
    "print(\"Absolute times for m={} and n={}:\\n\\nGaussian:\\t{:.2f}\\nExplicit inv:\\t{:.2f}\\nPseudo inv:\\t{:.2f}\".format(\n",
    "  m, n, t_gaussian.flatten()[-1], t_inv.flatten()[-1], t_pinv.flatten()[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: scipy solvers\n",
    "Add the following two solvers to your timing comparison above: \n",
    "- scipy.optimize.lsq_linear (linear least-squares with bound constraints)\n",
    "- scipy.optimize.least_squares (nonlinear least-squares with bound constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_lsq_linear = np.zeros_like(t_gaussian)\n",
    "t_lsq_nonlinear = np.zeros_like(t_gaussian)\n",
    "\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "@timing\n",
    "def lsq_linear(A, b):\n",
    "  raise NotImplementedError(\"Replace with your implementation!\")\n",
    "\n",
    "@timing\n",
    "def lsq_nonlinear(A, b):\n",
    "  raise NotImplementedError(\"Replace with your implementation!\")\n",
    "\n",
    "for i, m in enumerate(m_arr):\n",
    "  for j, n in enumerate(n_arr):\n",
    "    A = rng.normal(size=(m, n))\n",
    "    b = rng.normal(size=(m))\n",
    "\n",
    "    _, t_lsq_linear[i, j] = lsq_linear(A, b)\n",
    "    _, t_lsq_nonlinear[i, j] = lsq_nonlinear(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log2_mn2.flatten(), np.log2(t_gaussian.flatten()), '*', label=\"Gaussian elimination\")\n",
    "plt.plot(log2_mn2.flatten(), np.log2(t_inv.flatten()), '*', label=\"Explicit inverse\")\n",
    "plt.plot(log2_mn2.flatten(), np.log2(t_pinv.flatten()), '*', label=\"Pseudo-inverse\")\n",
    "plt.plot(log2_mn2.flatten(), np.log2(t_lsq_linear.flatten()), '*', label=\"lsq_linear\")\n",
    "plt.plot(log2_mn2.flatten(), np.log2(t_lsq_nonlinear.flatten()), '*', label=\"least_squares\")\n",
    "plt.xlabel(\"log($mn^2$)\")\n",
    "plt.ylabel(\"log(time)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Solution\n",
    "Click the dots to see a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_lsq_linear = np.zeros_like(t_gaussian)\n",
    "t_lsq_nonlinear = np.zeros_like(t_gaussian)\n",
    "\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "@timing\n",
    "def lsq_linear(A, b):\n",
    "  return opt.lsq_linear(A, b)\n",
    "\n",
    "@timing\n",
    "def lsq_nonlinear(A, b):\n",
    "  return opt.least_squares(fun=lambda x: A @ x - b, x0=np.zeros(n))\n",
    "\n",
    "for i, m in enumerate(m_arr):\n",
    "  for j, n in enumerate(n_arr):\n",
    "    A = rng.normal(size=(m, n))\n",
    "    b = rng.normal(size=(m))\n",
    "\n",
    "    _, t_lsq_linear[i, j] = lsq_linear(A, b)\n",
    "    _, t_lsq_nonlinear[i, j] = lsq_nonlinear(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log2_mn2.flatten(), np.log2(t_gaussian.flatten()), '*', label=\"Gaussian elimination\")\n",
    "plt.plot(log2_mn2.flatten(), np.log2(t_inv.flatten()), '*', label=\"Explicit inverse\")\n",
    "plt.plot(log2_mn2.flatten(), np.log2(t_pinv.flatten()), '*', label=\"Pseudo-inverse\")\n",
    "plt.plot(log2_mn2.flatten(), np.log2(t_lsq_linear.flatten()), '*', label=\"lsq_linear\")\n",
    "plt.plot(log2_mn2.flatten(), np.log2(t_lsq_nonlinear.flatten()), '*', label=\"least_squares\")\n",
    "plt.xlabel(\"log($mn^2$)\")\n",
    "plt.ylabel(\"log(time)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The takeaway from these experiments is that there is a lot to gain from picking the appropriate solver. In particular, it may be convenient to choose a general solver, but there is a price to pay in terms of the computational efficiency. Ultimately, the problem you're trying to solve determines what is a good tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: analytical properties\n",
    "- Is the objective function continuously differentiable? To what order?\n",
    "- Compute the Jacobian and Hessian. What is the Lipschitz constant? \n",
    "- What type of minimum should we expect to find?\n",
    "\n",
    "Hint: see, for instance, [the matrix cookbook](https://ece.uwaterloo.ca/~ece602/MISC/matrixcookbook.pdf) for how to compute derivatives of matrix expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Solution\n",
    "Click the dots to see a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a quadratic expression, hence it is infinitely differentiable (with derivatives of order higher than two being identically zero).\n",
    "\n",
    "First, we compute the gradient:\n",
    "\\begin{align*}\n",
    "\\nabla \\frac{1}{2m}\\|Ax-b\\|_2^2 &= \\frac{1}{2m}\\nabla ((x^\\top A^\\top A x-2(A^\\top b)^\\top x+b^\\top b))\\\\\n",
    "&=\\frac{1}{2m}((A^\\top A + A^\\top A)x-2A^\\top b)\\\\\n",
    "&= \\frac{1}{m}(A^\\top A x-A^\\top b)\n",
    "\\end{align*}\n",
    "Note that (i) the gradient is a column vector (assuming x is), and (ii) setting the gradient to zero yields the normal equations. For a scalar function, the Jacobian is the transpose of the gradient, i.e. a row vector. \n",
    "\n",
    "Next, we compute the Hessian as the gradient of the Jacobian:\n",
    "\\begin{align*}\n",
    "\\nabla^2 \\frac{1}{2m}\\|Ax-b\\|_2^2 &= \\nabla \\frac{1}{m}(A^\\top A x-A^\\top b)^\\top \\\\\n",
    "&= \\frac{1}{m}A^\\top A\n",
    "\\end{align*}\n",
    "\n",
    "For functions that are convex and twice differentiable, the Lipschitz constant is the largest eigenvalue of the Hessian. For our least-squares problem, that means that the Lipschitz constant is\n",
    "$$\n",
    "L=\\frac{1}{m}\\lambda_\\text{max}(A^\\top A)=\\frac{\\sigma_\\text{max}(A)^2}{m}=\\frac{\\|A\\|_2^2}{m}.\n",
    "$$ \n",
    "\n",
    "The Hessian is positive semidefinite, since for any $x\\in\\mathbb{R}^n$\n",
    "$$\n",
    "x^\\top A^\\top A x=(Ax)^\\top (Ax)=\\|Ax\\|_2^2\\geq 0.\n",
    "$$\n",
    "Thus, a solution $x^*$ is always a global minimizer (since the problem is convex) and furthermore the unique minimizer if $A$ is non-singular. Moreover, by Lemma 2.3 it is also $L$-smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "lipschitz = np.zeros_like(t_gaussian)\n",
    "\n",
    "for i, m in enumerate(m_arr):\n",
    "  for j, n in enumerate(n_arr):\n",
    "    A = rng.normal(size=(m, n))\n",
    "    lipschitz[i, j] = np.linalg.norm(A, ord=2)**2 / m\n",
    "print(lipschitz)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
